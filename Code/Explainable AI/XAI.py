# -*- coding: utf-8 -*-
"""Another copy of XAI- Classification of defects building.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JYnrQIKHyIiaYn8aQvu1FI0-AxLS9B_a
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Importing Liabraries"""

!pip install tensorflow opencv-python matplotlib scikit-learn

"""## Data Collection"""

import os
import cv2
import numpy as np
import pandas as pd

 # Path to the images folder
categories = ['Overweld', 'Porosity','Undercut', 'Underfilled']

# Initialize lists to hold the images and labels
images = []
labels = []

import os
import cv2
import numpy as np

# Define your paths
original_images_path = '/content/drive/MyDrive/Defects_welding/Combined'
categories = [ 'Overweld', 'Porosity','Undercut', 'Underfilled']
label_map = {0: 'Overweld', 1: 'Porosity', 2: 'Undercut', 3: 'Underfilled'}

# Initialize lists to hold the original and augmented images and labels
original_images = []
original_labels = []

# Load original images and labels
for category in categories:
    category_path = os.path.join(original_images_path, category)
    label = list(label_map.keys())[list(label_map.values()).index(category)]

    for img_name in os.listdir(category_path):
        img_path = os.path.join(category_path, img_name)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, (224, 224))  # Resize to match model input
            original_images.append(img)
            original_labels.append(label)

# Convert to numpy arrays
combined_images = np.array(original_images)
combined_labels = np.array(original_labels)

# Print combined dataset size
print(f"Combined dataset size: {len(combined_images)} images")
print(f"Combined labels size: {len(combined_labels)} labels")

image = np.array(combined_images)
label = np.array(combined_labels)

# Create a DataFrame for easy manipulation
data = pd.DataFrame({'image': list(image), 'label': label})

from matplotlib import pyplot as plt
categories = ['Overweld', 'Porosity', 'Undercut', 'Underfilled']
label_map = {0: 'Overweld', 1: 'Porosity', 2: 'Undercut', 3: 'Underfilled'}

# Plot the histogram
data['label'].plot(kind='hist', bins=20,title='Building Defects')
plt.gca().spines[['top', 'right']].set_visible(False)

# Set the x-axis tick labels to category names
plt.xticks(ticks=range(len(categories)), labels=categories)

# Show the plot
plt.show()

# Count occurrences of each label
label_counts = data['label'].value_counts()
print(label_counts)
print("------------------")
print(label_map)

combined_images.shape

combined_labels.shape

print(len(combined_images), len(combined_labels))

"""## Split the Data"""

from sklearn.model_selection import train_test_split

# Split combined dataset into training (70%), validation (20%), and test (10%)
X_train_val, X_test, y_train_val, y_test = train_test_split(combined_images, combined_labels, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)

# Print sizes of the splits
print(f"Training dataset size: {len(X_train)} images")
print(f"Validation dataset size: {len(X_val)} images")
print(f"Test dataset size: {len(X_test)} images")

print(f"Train/Val size: {len(X_train_val)}, Test size: {len(X_test)}")
print(f"Train size: {len(X_train)}, Val size: {len(X_val)}")

print(X_train[0])

"""## Data Preprocessing"""

# Normalize images
X_train = X_train.astype('float32') / 255.0
X_val = X_val.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# One-hot encode the labels
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
y_train = to_categorical(y_train, num_classes=4)
y_val = to_categorical(y_val, num_classes=4)
y_test = to_categorical(y_test, num_classes=4)

import matplotlib.pyplot as plt

# Display a few training images
for i in range(5):
    plt.imshow(X_train[i])
    plt.title(f"Label: {y_train[i]}")
    plt.show()

"""# Explainable AI

## SHAP
"""

!pip install shap lime opencv-python matplotlib pillow

from tensorflow.keras.models import load_model

model = load_model('/content/densenet_defect_classifier (7).h5')
model.summary()

import numpy as np
import cv2
import matplotlib.pyplot as plt

def load_images(img_paths):
    images = []
    for path in img_paths:
        img = cv2.imread(path)
        img = cv2.resize(img, (224, 224))
        img = img / 255.0  # Normalize
        images.append(img)
    return np.array(images)

img_paths = ['/content/WhatsApp Image 2025-02-11 at 17.38.47_fb82cdc9.jpg']  # Add more as needed
X_sample = load_images(img_paths)

!pip install --upgrade shap

print("Input image shape:", img_array.shape)
print("Background shape:", background.shape)
print("Background single image shape:", X_train[0].shape)

import cv2
import numpy as np
import shap
import matplotlib.pyplot as plt

# Load and normalize input image
img_path = '/content/13_Overweld.jpg' # Add more as needed

import shap
import numpy as np

# Loop through each image in the batch
for i in range(len(shap_values.values)):
    # Extract SHAP values for the i-th image in the batch
    shap_values_array = shap_values.values[i]  # Shape: (height, width, channels)

    # Sum or average across color channels to get a single value per pixel
    shap_values_2d = np.sum(shap_values_array, axis=2)  # or np.mean(...)

    # Visualize using shap.image_plot (2D SHAP values + Original image)
    shap.image_plot([shap_values_2d], pixel_values=X_sample[i])

import matplotlib.pyplot as plt
import matplotlib.image as mpimg # Import the necessary module

img_path = "/content/augmented_aug_3_60_augmented_aug_3_117_IMG-20240305-WA0101.jpg"

# Read the image data using mpimg.imread
img = mpimg.imread(img_path)

plt.imshow(img)
plt.title("SHAP Explanation")
plt.show() # Add plt.show() to display the plot

import shap
import numpy as np
img_paths = ['/content/augmented_aug_3_60_augmented_aug_3_117_IMG-20240305-WA0101.jpg']  # Add more as needed

# Load the images into an array
X_sample = load_images(img_paths)

img = cv2.imread(img_path)
img = cv2.resize(img, (224, 224))

# Normalize the image if necessary
img_array = np.expand_dims(img, axis=0) / 255.0

# Get background (for SHAP, you can average 10 images as you suggested)
background_images = np.random.randn(10, 224, 224, 3)  # Replace with actual background images
background_avg = np.mean(background_images, axis=0)

# Initialize the Image masker by passing the actual background image (not just the shape)
masker = shap.maskers.Image(mask_value=background_avg)

# Initialize the SHAP explainer with the background
explainer = shap.Explainer(model, masker)

# Calculate SHAP values for the image
shap_values = explainer(img_array)

# Loop through each image in the batch
for i in range(len(shap_values.values)):
    # Extract SHAP values for the i-th image in the batch
    shap_values_array = shap_values.values[i]  # Shape: (height, width, channels)

    # Sum or average across color channels to get a single value per pixel
    shap_values_2d = np.sum(shap_values_array, axis=2)

    # Visualize using shap.image_plot (2D SHAP values + Original image)
    shap.image_plot([shap_values_2d], pixel_values=X_sample[i])

import shap
import numpy as np  # Import numpy if you haven't already

# Access the values array from the Explanation object
shap_values_array = shap_values.values

# Assuming shap_values_array is a 5-dimensional array (batch, height, width, channels, classes)
# Average or sum the SHAP values across channels to get a single value for each pixel
shap_values_2d = np.sum(shap_values_array[0], axis=2)  # Summing across channels (axis 2)

# Now, plot using the 2D SHAP values and the original image
shap.image_plot(shap_values_2d, pixel_values=X_sample[0])

"""## GRAD- CAM"""

import numpy as np
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing import image

# Function to preprocess the image (resize, normalize, etc.)
def preprocess_image(img_path, target_size=(224, 224)):
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array = img_array / 255.0  # Normalize
    return img_array

def get_gradcam_heatmap(model, img_array, last_conv_layer_name='conv5_block16_concat', pred_index=None):
    # Create a model that maps the input image to the activations of the last conv layer & predictions
    grad_model = tf.keras.models.Model(
        inputs=model.input,
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Forward pass + Gradient Calculation
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)  # img_array is passed here
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])  # Get the index of the most probable class
        loss = predictions[:, pred_index]

    # Compute gradients
    grads = tape.gradient(loss, conv_outputs)

    # Pool gradients across spatial dimensions
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # Get the output feature map
    conv_outputs = conv_outputs[0]

    # Weight each channel by corresponding gradients
    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)

    # Apply ReLU and normalize
    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)

    return heatmap.numpy(), predictions[0]

categories = {0: "Overweld", 1:"Porosity", 2: "Undercut", 3: "Underfilled"}

def display_gradcam(img_path, model, last_conv_layer_name='conv5_block16_concat'):
    img_array = preprocess_image(img_path)

    # Generate the Grad-CAM heatmap and prediction
    heatmap, prediction = get_gradcam_heatmap(model, img_array, last_conv_layer_name)
        # Load and prepare the original image for overlay
    img = cv2.imread(img_path)
    img = cv2.resize(img, (224, 224))  # Resize to match the input size of the model
    img = np.array(img)

    # First, display the real image
    plt.figure(figsize=(8,6))
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title('Original Image')
    plt.show()

    # Resize the heatmap to match the original image size
    heatmap_resized = cv2.resize(heatmap, (224, 224))
    heatmap_resized = np.uint8(255 * heatmap_resized)

    # Create a color gradient for heatmap visualization (Red, Yellow, Green, Blue)
    heatmap_colored = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)
    overlayed_img = cv2.addWeighted(img, 0.6, heatmap_colored, 0.4, 0)

    # Display the overlay image
    plt.figure(figsize=(8,6))
    plt.imshow(cv2.cvtColor(overlayed_img, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title('Overlay Image')
    predicted_class = np.argmax(prediction)
    prediction_class_name = categories[predicted_class]
    confidence = prediction[predicted_class] * 100
    plt.title(f"Predicted Class: {prediction_class_name}, Confidence: {confidence:.2f}%")

    # Show the heatmap as a pixel box with the red color indicating important features
    plt.figure(figsize=(8, 6))
    plt.imshow(cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.title("Grad-CAM Heatmap")
    plt.show()

    # Display the classification result
    predicted_class = np.argmax(prediction)
    prediction_class_name = categories[predicted_class]
    confidence = prediction[predicted_class] * 100
    print(f"Predicted Class: {prediction_class_name}, Confidence: {confidence:.2f}%")

def gridget_gradcam_heatmap(model, img_array, last_conv_layer_name='conv5_block16_concat', pred_index=None):
    # Create a model that maps the input image to the activations of the last conv layer & predictions
    grad_model = tf.keras.models.Model(
        inputs=model.input,
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Forward pass + Gradient Calculation
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array) # img_array is passed here
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        loss = predictions[:, pred_index]

    # Compute gradients
    grads = tape.gradient(loss, conv_outputs)

    # Pool gradients across spatial dimensions
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # Get the output feature map
    conv_outputs = conv_outputs[0]

    # Weight each channel by corresponding gradients
    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)

    # Apply ReLU and normalize
    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)

    return heatmap.numpy()

# Example usage: Load and preprocess your image first
img_path = '/content/13_Overweld.jpg'   # Path to your image
display_gradcam(img_path, model)  # Pass the pre-trained model here
# Update to the path of your image file
img_array = preprocess_image(img_path) # Use preprocess_image function to load image as numpy array

# Then generate the heatmap
heatmap = gridget_gradcam_heatmap(model, img_array)  # You can specify `last_conv_layer_name` if using other DenseNet versions

# Display heatmap
plt.matshow(heatmap)
plt.title('Grid Heatmap')
plt.colorbar()
plt.show()

# Example usage: Load and preprocess your image first
img_path = '/content/augmented_aug_2_29_augmented_aug_2_117_IMG-20240305-WA0075.jpg'   # Path to your image
display_gradcam(img_path, model)  # Pass the pre-trained model here
# Update to the path of your image file
img_array = preprocess_image(img_path) # Use preprocess_image function to load image as numpy array

# Then generate the heatmap
heatmap = gridget_gradcam_heatmap(model, img_array)  # You can specify `last_conv_layer_name` if using other DenseNet versions

# Display heatmap
plt.matshow(heatmap)
plt.title('Grid Heatmap')
plt.colorbar()
plt.show()

# Example usage: Load and preprocess your image first
img_path = '/content/augmented_aug_3_60_augmented_aug_3_117_IMG-20240305-WA0101.jpg'   # Path to your image
display_gradcam(img_path, model)  # Pass the pre-trained model here
# Update to the path of your image file
img_array = preprocess_image(img_path) # Use preprocess_image function to load image as numpy array

# Then generate the heatmap
heatmap = gridget_gradcam_heatmap(model, img_array)  # You can specify `last_conv_layer_name` if using other DenseNet versions

# Display heatmap
plt.matshow(heatmap)
plt.title('Grid Heatmap')
plt.colorbar()
plt.show()

# Example usage: Load and preprocess your image first
img_path = '/content/augmented_aug_0_24_augmented_aug_0_108_IMG-20240305-WA0042.jpg'   # Path to your image
display_gradcam(img_path, model)  # Pass the pre-trained model here
# Update to the path of your image file
img_array = preprocess_image(img_path) # Use preprocess_image function to load image as numpy array

# Then generate the heatmap
heatmap = gridget_gradcam_heatmap(model, img_array)  # You can specify `last_conv_layer_name` if using other DenseNet versions

# Display heatmap
plt.matshow(heatmap)
plt.title('Grid Heatmap')
plt.colorbar()
plt.show()

# Example usage: Load and preprocess your image first
img_path = '/content/WhatsApp Image 2025-02-11 at 17.38.46_5ba57003.jpg'   # Path to your image
display_gradcam(img_path, model)  # Pass the pre-trained model here
# Update to the path of your image file
img_array = preprocess_image(img_path) # Use preprocess_image function to load image as numpy array

# Then generate the heatmap
heatmap = gridget_gradcam_heatmap(model, img_array)  # You can specify `last_conv_layer_name` if using other DenseNet versions

# Display heatmap
plt.matshow(heatmap)
plt.title('Grid Heatmap')
plt.colorbar()
plt.show()

# Example usage: Load and preprocess your image first
img_path = '/content/IMG-20240305-WA0043.jpg'   # Path to your image
display_gradcam(img_path, model)  # Pass the pre-trained model here
# Update to the path of your image file
img_array = preprocess_image(img_path) # Use preprocess_image function to load image as numpy array

# Then generate the heatmap
heatmap = gridget_gradcam_heatmap(model, img_array)  # You can specify `last_conv_layer_name` if using other DenseNet versions

# Display heatmap
plt.matshow(heatmap)
plt.title('Grid Heatmap')
plt.colorbar()
plt.show()

"""## LIME"""

from lime import lime_image
from skimage.segmentation import mark_boundaries
import matplotlib.pyplot as plt
import cv2
import numpy as np

explainer = lime_image.LimeImageExplainer()

def model_predict(images):
    """
    Predict the class of an input image using a pre-trained model.
    """
    images = np.array(images) / 255.0  # Normalize
    return model.predict(images)

# Load and preprocess image
img = cv2.imread('/content/WhatsApp Image 2025-02-11 at 17.38.47_fb82cdc9.jpg')
img = cv2.resize(img, (224, 224))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Get LIME explanation
explanation = explainer.explain_instance(
    img.astype('double'),
    model_predict,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)

# Extract LIME mask
temp, mask = explanation.get_image_and_mask(
    label=explanation.top_labels[0],
    positive_only=True,
    num_features=10,
    hide_rest=True
)

# Plot original image and LIME explanation side by side
fig, ax = plt.subplots(1, 2, figsize=(10, 5))

# Original image
ax[0].imshow(img_rgb)
ax[0].set_title("Original Image")
ax[0].axis('off')

# LIME Explanation with mask overlay
ax[1].imshow(mark_boundaries(temp, mask))
ax[1].set_title("LIME Explanation")
ax[1].axis('off')

plt.tight_layout()
plt.show()

from lime import lime_image
from skimage.segmentation import mark_boundaries
import matplotlib.pyplot as plt
import cv2
import numpy as np

explainer = lime_image.LimeImageExplainer()

def model_predict(images):
    images = images / 255.0  # Normalize if needed
    return model.predict(images)

def explain_with_lime(img_path, class_labels):
    # Load image
    img = cv2.imread(img_path)
    img = cv2.resize(img, (224, 224))

    # Predict class
    img_array = np.expand_dims(img, axis=0)
    predictions = model_predict(img_array)
    predicted_class = np.argmax(predictions[0])
    predicted_label = class_labels[predicted_class]

    print(f"Predicted Class: {predicted_class} â†’ {predicted_label}")

    # Get LIME explanation
    explanation = explainer.explain_instance(
        img.astype('double'),
        model_predict,
        top_labels=1,
        hide_color=0,
        num_samples=1000
    )

    # Visualize LIME Explanation
    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,
        num_features=10,
        hide_rest=True
    )

    # Show the original image & LIME explanation
    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title("Original Image")

    plt.subplot(1, 2, 2)
    plt.imshow(mark_boundaries(temp, mask))
    plt.title(f"LIME Explanation: {predicted_label}")

    plt.show()

from lime import lime_image
from skimage.segmentation import mark_boundaries
import matplotlib.pyplot as plt
import cv2
import numpy as np

explainer = lime_image.LimeImageExplainer()

def model_predict(images):
    """
    Predict the class of an input image using a pre-trained model.
    """
    images = np.array(images) / 255.0  # Normalize
    return model.predict(images)

# Load and preprocess image
img = cv2.imread('/content/WhatsApp Image 2025-02-11 at 17.38.47_fb82cdc9.jpg')
img = cv2.resize(img, (224, 224))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Get LIME explanation
explanation = explainer.explain_instance(
    img.astype('double'),
    model_predict,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)

# Extract LIME mask
temp, mask = explanation.get_image_and_mask(
    label=explanation.top_labels[0],
    positive_only=True,
    num_features=10,
    hide_rest=True
)

# Plot original image and LIME explanation side by side
fig, ax = plt.subplots(1, 2, figsize=(10, 5))

# Original image
ax[0].imshow(img_rgb)
ax[0].set_title("Original Image")
ax[0].axis('off')

# LIME Explanation with mask overlay
ax[1].imshow(mark_boundaries(temp, mask))
ax[1].set_title("LIME Explanation")
ax[1].axis('off')

plt.tight_layout()
plt.show()

# Load and preprocess image
img = cv2.imread('/content/augmented_aug_0_24_augmented_aug_0_108_IMG-20240305-WA0042.jpg')
img = cv2.resize(img, (224, 224))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)


# Get LIME explanation
explanation = explainer.explain_instance(
    img.astype('double'),
    model_predict,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)

# Extract LIME mask
temp, mask = explanation.get_image_and_mask(
    label=explanation.top_labels[0],
    positive_only=True,
    num_features=10,
    hide_rest=True
)

# Plot original image and LIME explanation side by side
fig, ax = plt.subplots(1, 2, figsize=(10, 5))

# Original image
ax[0].imshow(img_rgb)
ax[0].set_title("Original Image")
ax[0].axis('off')

# LIME Explanation with mask overlay
ax[1].imshow(mark_boundaries(temp, mask))
ax[1].set_title("LIME Explanation")
ax[1].axis('off')

plt.tight_layout()
plt.show()

# Load and preprocess image
img = cv2.imread('/content/augmented_aug_1_17_augmented_aug_1_127_IMG-20240305-WA0073.jpg')
img = cv2.resize(img, (224, 224))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)


# Get LIME explanation
explanation = explainer.explain_instance(
    img.astype('double'),
    model_predict,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)

# Extract LIME mask
temp, mask = explanation.get_image_and_mask(
    label=explanation.top_labels[0],
    positive_only=True,
    num_features=10,
    hide_rest=True
)


# Plot original image and LIME explanation side by side
fig, ax = plt.subplots(1, 2, figsize=(10, 5))

# Original image
ax[0].imshow(img_rgb)
ax[0].set_title("Original Image")
ax[0].axis('off')

# LIME Explanation with mask overlay
ax[1].imshow(mark_boundaries(temp, mask))
ax[1].set_title("LIME Explanation")
ax[1].axis('off')

plt.tight_layout()
plt.show()

